import pandas as pd
import os
import glob
import pickle
from config import DATA_DIR

class DatabaseManager:
    def __init__(self):
        self.data_dir = DATA_DIR
        self.pop_df = None
        self.tax_df = None
        self.mrt_df = None
        self.rent_df = None
        self.is_loaded = False

    def load_data_lazily(self):
        """
        Loads data only when needed, or in background.
        Uses pickle cache to speed up subsequent reloads.
        Optimized for LOW MEMORY usage (Render Free Tier 512MB).
        """
        if self.is_loaded:
            return

        cache_file = os.path.join(self.data_dir, "db_cache_v2.pkl")
        
        # Try load from cache
        if os.path.exists(cache_file):
            print("âš¡ ç™¼ç¾å¿«å–æª”æ¡ˆ (v2)ï¼Œæ­£åœ¨å¿«é€Ÿè¼‰å…¥è³‡æ–™åº«...")
            try:
                # Use 'rb' and load
                with open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                    self.pop_df = data.get('pop')
                    self.tax_df = data.get('tax')
                    self.mrt_df = data.get('mrt')
                    self.rent_df = data.get('rent')
                self.is_loaded = True
                print(f"âœ… è³‡æ–™åº«è¼‰å…¥å®Œæˆ (ä¾†è‡ªå¿«å–)")
                return
            except Exception as e:
                print(f"å¿«å–è¼‰å…¥å¤±æ•—ï¼Œå°‡é‡æ–°è®€å–åŸå§‹æª”: {e}")

        print("ğŸ“¥ æ­£åœ¨è®€å–åŸå§‹ Excel æª”æ¡ˆ (è¨˜æ†¶é«”å„ªåŒ–æ¨¡å¼)...")
        
        # 1. Population (Only need specific cols)
        try:
            pop_file = os.path.join(self.data_dir, "113å¹´å…¨å°å„æ‘é‡Œæ€§åˆ¥äººå£çµ±è¨ˆ.xlsx")
            if os.path.exists(pop_file):
                # Columns usually: çµ±è¨ˆå¹´, å€åŸŸåˆ¥, æ‘é‡Œåç¨±, ç¸½äººå£, ç”·, å¥³
                # Try to load only relevant ones. But headers might vary.
                # Safe approach: Load all but use 'dtype' to optimize
                self.pop_df = pd.read_excel(pop_file) 
                self.pop_df.columns = self.pop_df.columns.str.strip()
                # Determine relevant columns
                cols_to_keep = [c for c in self.pop_df.columns if c in ['å€åŸŸåˆ¥', 'æ‘é‡Œåç¨±', 'å¥³', 'ç”·']]
                self.pop_df = self.pop_df[cols_to_keep].dropna()
                print(f"ğŸ“Š [Pop] Loaded. {len(self.pop_df)} rows.")
        except Exception as e: print(f"Pop Load Error: {e}")

        # 2. Tax (Only need Tax Payers & Median)
        try:
            tax_file = os.path.join(self.data_dir, "111å¹´åº¦ç¶œç¨…æ‰€å¾—æ‡‰ç´ç¨…é¡åŠç¨…ç‡å„ç¸£å¸‚ç”³å ±çµ±è¨ˆè¡¨ (2).xlsx")
            if os.path.exists(tax_file):
                self.tax_df = pd.read_excel(tax_file)
                self.tax_df.columns = self.tax_df.columns.str.strip()
                # Check known columns
                keep = ['ç¸£å¸‚åˆ¥', 'æ‘é‡Œ', 'ç´ç¨…å–®ä½(æˆ¶)', 'ä¸­ä½æ•¸']
                actual_keep = [c for c in keep if c in self.tax_df.columns]
                self.tax_df = self.tax_df[actual_keep].dropna()
                print(f"ğŸ“Š [Tax] Loaded. {len(self.tax_df)} rows.")
        except Exception as e: print(f"Tax Load Error: {e}")

        # 3. MRT (Flow matrix)
        try:
            mrt_file = os.path.join(self.data_dir, "202510å„ç«™é€²å‡ºé‡çµ±è¨ˆ.xlsx")
            if os.path.exists(mrt_file):
                # ... Existing Header Logic ...
                temp_df = pd.read_excel(mrt_file, header=None, nrows=10) # READ ONLY 10 ROWS FIRST
                header_row_idx = 0
                for i in range(10):
                    row_str = temp_df.iloc[i].astype(str).values
                    if any("A18" in s or "A19" in s or "æ—¥æœŸ" in s or "è»Šç«™" in s for s in row_str):
                        header_row_idx = i
                        break
                
                self.mrt_df = pd.read_excel(mrt_file, header=header_row_idx)
                self.mrt_df.columns = self.mrt_df.columns.astype(str).str.strip()
                # MRT df is usually small (rows = days of month, cols = stations). Keep as is.
        except Exception as e: print(f"âŒ MRT Load Error: {e}")
            
        # 4. Rent (HEAVY! Optimization Critical)
        rent_files = glob.glob(os.path.join(self.data_dir, "å…¨å°ç§Ÿé‡‘*.xls*"))
        rent_frames = []
        
        # Define Columns we absolutely need
        # 'é„‰é®å¸‚å€', 'åœŸåœ°å€æ®µä½ç½®å»ºç‰©å€æ®µé–€ç‰Œ', 'ç§»è½‰å±¤æ¬¡', 'ç¸½é¡å…ƒ', 'å–®åƒ¹å…ƒå¹³æ–¹å…¬å°º', 'å»ºç‰©ç¸½é¢ç©å¹³æ–¹å…¬å°º'
        required_cols = ['é„‰é®å¸‚å€', 'åœŸåœ°å€æ®µä½ç½®å»ºç‰©å€æ®µé–€ç‰Œ', 'ç§»è½‰å±¤æ¬¡', 'ç¸½é¡å…ƒ', 'å–®åƒ¹å…ƒå¹³æ–¹å…¬å°º', 'å»ºç‰©ç¸½é¢ç©å¹³æ–¹å…¬å°º']
        
        for rf in rent_files:
            try:
                # Use Pandas usecols callable to handle slight name variations (e.g. 'å–®åƒ¹å…ƒå¹³æ–¹å…¬å°º' vs 'å–®åƒ¹')
                # But generic 'usecols' logic is tricky if headers vary.
                # Strategy: Read headers first? Too many files. 
                # Strategy: Read File. Select columns immediately. Drop NaN. Append.
                
                df = pd.read_excel(rf) # Read full
                df.columns = df.columns.str.strip()
                
                # Filter Columns
                found_cols = [c for c in df.columns if c in required_cols]
                
                if 'é„‰é®å¸‚å€' in found_cols: # Basic check
                    df_mini = df[found_cols].copy()
                    
                    # Drop rows where Area is 0 to save space
                    if 'å»ºç‰©ç¸½é¢ç©å¹³æ–¹å…¬å°º' in df_mini.columns:
                        df_mini = df_mini[df_mini['å»ºç‰©ç¸½é¢ç©å¹³æ–¹å…¬å°º'] > 0]
                        
                    rent_frames.append(df_mini)
                
                # Force Garbage Collection
                del df
            except: pass
        
        if rent_frames:
            self.rent_df = pd.concat(rent_frames, ignore_index=True)
            print(f"âœ… [Rent] Loaded. {len(self.rent_df)} rows. (Mem Optimized)")
        else:
            print("âš ï¸ [Rent] No files loaded.")

        self.is_loaded = True
        
        # Save Optimized Cache
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump({
                    'pop': self.pop_df,
                    'tax': self.tax_df,
                    'mrt': self.mrt_df,
                    'rent': self.rent_df
                }, f)
            print("ğŸ’¾ å·²å»ºç«‹å„ªåŒ–ç‰ˆå¿«å– db_cache_v2.pkl")
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•å»ºç«‹å¿«å–: {e}")


    def get_village_data(self, city, district, village, mrt_station_name=None):
        if not self.is_loaded: self.load_data_lazily()
        
        result = {
            "Population": 0, "Male_Pop": 0, "Female_Pop": 0,
            "Tax_Payers": 0, "Income_Median": 0,
            "MRT_Station": mrt_station_name, "MRT_Flow": 0,
            "Rent_1F_Avg": 0, "Rent_Upper_Avg": 0, "Rent_Advice": "" 
        }
        
        norm_city = city.replace('å°', 'è‡º') if city else ""

        # Logic same as before, just ensuring loaded
        if self.pop_df is not None:
            try:
                # 1. Normalize Inputs
                search_city = norm_city
                search_dist = district.strip()
                search_village = village.strip()
                
                print(f"ğŸ” [DB Debug] æœå°‹ç›®æ¨™: {search_city} | {search_dist} | {search_village}")

                # 2. Filter Population (Columns: ['å€åŸŸåˆ¥', 'æ‘é‡Œåç¨±', 'å¥³', 'ç”·', 'ç¸½äººå£'])
                # 'å€åŸŸåˆ¥' likely contains "City District" e.g. "è‡ºåŒ—å¸‚æ¾å±±å€" or just "æ¾å±±å€" depending on source.
                # We will use string contain search on 'å€åŸŸåˆ¥' for both City and District to be safe.
                
                df = self.pop_df.astype(str)
                
                # Loose Match Strategy:
                # Check if 'å€åŸŸåˆ¥' contains "District" AND ("City" OR is implicit)
                # Usually 'å€åŸŸåˆ¥' is uniquely identifying like 'æ–°åŒ—å¸‚æ¿æ©‹å€'
                
                keywords = [search_dist]
                if len(search_city) > 2: keywords.append(search_city[-2:]) # "åŒ—å¸‚"
                
                # Custom mask builder
                loc_mask = df['å€åŸŸåˆ¥'].apply(lambda x: all(k in x for k in keywords))
                
                # Village Match
                village_mask = df['æ‘é‡Œåç¨±'] == search_village
                
                q = df[loc_mask & village_mask]
                
                # Retry Fuzzy Village
                if q.empty and len(search_village) > 2:
                     short_village = search_village.replace("é‡Œ", "").replace("æ‘", "")
                     village_mask_fuzzy = df['æ‘é‡Œåç¨±'].str.contains(short_village)
                     q = df[loc_mask & village_mask_fuzzy]

                if not q.empty:
                    row = q.iloc[0]
                    m = int(float(row.get('ç”·', 0))) # Column is 'ç”·'
                    f = int(float(row.get('å¥³', 0))) # Column is 'å¥³'
                    result['Population'] = m + f
                    result['Male_Pop'] = m
                    result['Female_Pop'] = f
                    print(f"âœ… [DB Success] æ‰¾åˆ°äººå£æ•¸æ“š: {result['Population']} äºº")
                else:
                    print(f"âš ï¸ [DB Warning] æ‰¾ä¸åˆ°äººå£æ•¸æ“š (æœå°‹: {search_city}{search_dist} - {search_village})")

            except Exception as e: 
                print(f"âŒ [DB Error] Population lookup failed: {e}")

        if self.tax_df is not None:
            try:
                # Tax Columns: ['ç¸£å¸‚åˆ¥', 'æ‘é‡Œ', 'ç´ç¨…å–®ä½(æˆ¶)', 'ç¶œåˆæ‰€å¾—ç¸½é¡', 'å¹³å‡æ•¸', 'ä¸­ä½æ•¸'...]
                df = self.tax_df.astype(str)
                
                search_city_short = norm_city.replace("å°", "").replace("è‡º", "")
                
                # Filter
                if not search_village:
                    print("âš ï¸ [DB Debug] æ‘é‡Œåç¨±ç‚ºç©ºï¼Œè·³éç´ç¨…æŸ¥è©¢ä»¥é¿å…èª¤åˆ¤")
                else:
                    # Proceed with tax lookup only if village exists
                    city_mask = df['ç¸£å¸‚åˆ¥'].str.contains(search_city_short)
                    # Tax file often splits City/District or merges them? 
                    # If 'è¡Œæ”¿å€' column is MISSING (as seen in logs), then 'ç¸£å¸‚åˆ¥' or 'æ‘é‡Œ' might contain it?
                    # Actually logs showed: ['ç¸£å¸‚åˆ¥', 'æ‘é‡Œ', 'ç´ç¨…å–®ä½(æˆ¶)'...] -> WHERE IS DISTRICT?
                    # Maybe 'æ‘é‡Œ' contains "District Village"? Or 'ç¸£å¸‚åˆ¥' contains "City District"?
                    # Let's try matching District in 'ç¸£å¸‚åˆ¥' OR 'æ‘é‡Œ' just to be safe.
                    # Actually standard MOF tax data usually has "ç¸£å¸‚", "è¡Œæ”¿å€", "æ‘é‡Œ". 
                    # If only "ç¸£å¸‚åˆ¥" corresponds to City? And maybe "è¡Œæ”¿å€" is missing from print?
                    # Wait, User logs: ['ç¸£å¸‚åˆ¥', 'æ‘é‡Œ', 'ç´ç¨…å–®ä½(æˆ¶)'...] -> It seems District column is named something else or missing!
                    # Ah, standard file often has "è¡Œæ”¿å€åˆ¥" or merged. 
                    # Let's assume 'ç¸£å¸‚åˆ¥' might be 'æ–°åŒ—å¸‚æ¿æ©‹å€' or we search district in 'æ‘é‡Œ' (unlikely).
                    # Let's try searching District in 'ç¸£å¸‚åˆ¥' first (common in some files).
                    
                    dist_mask = df['ç¸£å¸‚åˆ¥'].str.contains(search_dist) | df['æ‘é‡Œ'].str.contains(search_dist)
                    
                    village_mask = df['æ‘é‡Œ'] == search_village
                    
                    q = df[city_mask & dist_mask & village_mask]
                    
                    if q.empty:
                         short_village = search_village.replace("é‡Œ", "").replace("æ‘", "")
                         village_mask_fuzzy = df['æ‘é‡Œ'].str.contains(short_village)
                         q = df[city_mask & dist_mask & village_mask_fuzzy]

                    if not q.empty:
                        row = q.iloc[0]
                        result['Tax_Payers'] = int(float(row.get('ç´ç¨…å–®ä½(æˆ¶)', 0)))
                        result['Income_Median'] = float(row.get('ä¸­ä½æ•¸', 0))
                        print(f"âœ… [DB Success] æ‰¾åˆ°ç´ç¨…æˆ¶æ•¸: {result['Tax_Payers']}")
                    else:
                        print(f"âš ï¸ [DB Warning] æ‰¾ä¸åˆ°ç´ç¨…æ•¸æ“š")
            except Exception as e:
                print(f"âŒ [DB Error] Tax lookup failed: {e}")

        if self.mrt_df is not None and mrt_station_name:
            # Clean: "æ·é‹æ±Ÿå­ç¿ ç«™" -> "æ±Ÿå­ç¿ "
            clean_station = mrt_station_name.replace("æ·é‹", "").replace("ç«™", "")
            
            # Search COLUMNS not Rows
            target_col = None
            
            # Normalize Columns for search
            cols = self.mrt_df.columns
            
            # 1. Exact Match on Clean
            if clean_station in cols:
                target_col = clean_station
            
            # 2. Try adding "ç«™" (e.g. "å°åŒ—è»Šç«™")
            elif clean_station + "ç«™" in cols:
                target_col = clean_station + "ç«™"

            # 3. Robust Fuzzy Match
            import re
            def normalize_station_name(name):
                # Remove English letters, Numbers, Spaces, "ç«™", "æ·é‹"
                # Keep only Chinese characters mostly
                s = str(name)
                s = re.sub(r'[A-Za-z0-9\s]', '', s) # Remove Code A19
                s = s.replace("æ·é‹", "").replace("ç«™", "").replace("è»Šç«™", "")
                return s

            target_clean = normalize_station_name(clean_station)
            
            for col in cols:
                col_clean = normalize_station_name(col)
                if not target_clean or not col_clean: continue
                
                # Check exact match of "core" name
                if target_clean == col_clean:
                    target_col = col
                    break
                # Check contains
                if target_clean in col_clean:
                     target_col = col
                     break
            
            if target_col:
                print(f"âœ… [DB Success] æ‰¾åˆ°æ·é‹ç«™æ•¸æ“š: {target_col} (åŸå§‹æœå°‹: {clean_station})")
                # Sum the column, ignoring non-numeric (first row usually ok if skipped by header)
                try:
                    # Convert to numeric, coerce errors to NaN, then sum
                    result['MRT_Flow'] = int(pd.to_numeric(self.mrt_df[target_col], errors='coerce').sum())
                    print(f"   -> æµé‡: {result['MRT_Flow']}")
                except Exception as e:
                    print(f"âš ï¸ Calculation Error: {e}")
                    result['MRT_Flow'] = 0
            
            else:
                print(f"âš ï¸ [DB Warning] æ‰¾ä¸åˆ°æ·é‹ç«™: {clean_station} (Normalized Target: {target_clean})")
                print(f"   -> First 5 Cols Normalized: {[normalize_station_name(c) for c in list(cols)[:5]]}")
            


        return result

    def get_rental_analysis(self, city, district, address_road):
        if not self.is_loaded: self.load_data_lazily()
        
        stats = {
            "1F_Count": 0, "1F_Min": 0, "1F_Max": 0, "1F_Avg": 0,
            "Upper_Count": 0, "Upper_Avg": 0,
            "Estimated_Range": "ç„¡æ•¸æ“š",
            "Data_Source_Count": 0
        }
        
        if self.rent_df is None or self.rent_df.empty:
            return stats

        try:
            # Filter Logic (Simpified for brevity, assume Logic from previous step)
            # 1. District
            mask = (self.rent_df['é„‰é®å¸‚å€'] == district)
            df_dist = self.rent_df[mask].copy()
            if df_dist.empty: return stats

            # 2. Price/Ping
            price_col = 'ç¸½é¡å…ƒ' if 'ç¸½é¡å…ƒ' in df_dist.columns else 'å–®åƒ¹å…ƒå¹³æ–¹å…¬å°º'
            area_col = 'å»ºç‰©ç¸½é¢ç©å¹³æ–¹å…¬å°º'
            df_dist = df_dist[df_dist[area_col] > 0]
            df_dist['Price_Per_Ping'] = df_dist[price_col] / (df_dist[area_col] * 0.3025)

            # 3. Road
            addr_col = 'åœŸåœ°å€æ®µä½ç½®å»ºç‰©å€æ®µé–€ç‰Œ'
            if address_road:
                df_road = df_dist[df_dist[addr_col].astype(str).str.contains(address_road)].copy()
                final_df = df_road if len(df_road) >= 3 else df_dist
            else:
                final_df = df_dist

            stats['Data_Source_Count'] = len(final_df)
            
            # 4. Floor
            floor_col = 'ç§»è½‰å±¤æ¬¡'
            mask_1f = final_df[floor_col].astype(str).str.contains('ä¸€å±¤|1å±¤')
            df_1f = final_df[mask_1f]
            df_upper = final_df[~mask_1f]

            if not df_1f.empty:
                prices = df_1f['Price_Per_Ping']
                stats['1F_Count'] = len(df_1f)
                stats['1F_Avg'] = int(prices.mean())
            
            if not df_upper.empty:
                prices = df_upper['Price_Per_Ping']
                stats['Upper_Count'] = len(df_upper)
                stats['Upper_Avg'] = int(prices.mean())

            # 5. Advice
            if stats['1F_Count'] >= 3:
                low = int(df_1f['Price_Per_Ping'].quantile(0.25))
                high = int(df_1f['Price_Per_Ping'].quantile(0.75))
                stats['Estimated_Range'] = f"{low} ~ {high}"
            elif stats['Upper_Count'] > 0:
                est = int(stats['Upper_Avg'] * 1.6)
                stats['Estimated_Range'] = f"{int(est*0.9)} ~ {int(est*1.1)} (æ¨ä¼°)"
        except Exception as e:
            print(f"Error in rent analysis: {e}")

        return stats
